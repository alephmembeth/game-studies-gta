{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a17531f",
   "metadata": {},
   "source": [
    "# Set Up Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "# System and file handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing and math\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Notebook display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "#################\n",
    "# CONFIGURATION #\n",
    "#################\n",
    "\n",
    "# Set path to corpus directory\n",
    "base_path = r'/Users/amb/Desktop/Corpus'\n",
    "\n",
    "# Configure global display settings for pandas\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "\n",
    "#########################\n",
    "# CONSPIRACY CATEGORIES #\n",
    "#########################\n",
    "conspiracy_categories = {\n",
    "    \"Governments and Elites\": [\n",
    "       \"agency\",\n",
    "        \"authorit*\",\n",
    "        \"black ops\",\n",
    "        \"bureau*\",\n",
    "        \"cabal*\",\n",
    "        \"clandestine\",\n",
    "        \"corrupt*\",\n",
    "        \"deep state\",\n",
    "        \"elite*\",\n",
    "        \"enlighten*\",\n",
    "        \"establishment\",\n",
    "        \"globalist*\",\n",
    "        \"government*\",\n",
    "        \"illuminati*\",\n",
    "        \"initiate*\",\n",
    "        \"intelligence\",\n",
    "        \"jew*\",\n",
    "        \"mason*\",\n",
    "        \"new world order\",\n",
    "        \"police state\",\n",
    "        \"secret society\",\n",
    "        \"surveil*\"\n",
    "    ],\n",
    "    \n",
    "    \"Technology and Biology\": [\n",
    "        \"bio-weapon*\",\n",
    "        \"chemical*\",\n",
    "        \"chemtrail*\",\n",
    "        \"chip*\",\n",
    "        \"clon*\",\n",
    "        \"dna\",\n",
    "        \"fluoride\",\n",
    "        \"frequency\",\n",
    "        \"genetic*\",\n",
    "        \"implant*\",\n",
    "        \"mk-ultra\",\n",
    "        \"nanobot*\",\n",
    "        \"pandem*\",\n",
    "        \"pathogen*\",\n",
    "        \"poison*\",\n",
    "        \"radiation\",\n",
    "        \"satellite*\",\n",
    "        \"toxic*\",\n",
    "        \"vaccin*\",\n",
    "        \"virus\"\n",
    "    ],\n",
    "    \n",
    "    \"Information and Disinformation\": [\n",
    "        \"brainwash*\",\n",
    "        \"classified\",\n",
    "        \"cover-up\",\n",
    "        \"decept*\",\n",
    "        \"distort*\",\n",
    "        \"fake*\",\n",
    "        \"hoax*\",\n",
    "        \"indoctrinate*\",\n",
    "        \"manipulate*\",\n",
    "        \"masses\",\n",
    "        \"media\",\n",
    "        \"paranoi*\",\n",
    "        \"red pil\",\n",
    "        \"propaganda\",\n",
    "        \"scripted\",\n",
    "        \"sheep*\",\n",
    "        \"spie*\",\n",
    "        \"staged\",\n",
    "        \"subliminal*\",\n",
    "        \"top-secret\",\n",
    "        \"truth\",\n",
    "        \"unveil*\",\n",
    "        \"wiretap*\"\n",
    "    ],\n",
    "    \n",
    "    \"Extraterrestrial and Paranormal\": [\n",
    "        \"abduct*\",\n",
    "        \"alien*\",\n",
    "        \"ancient\",\n",
    "        \"anomal*\",\n",
    "        \"apocalyp*\",\n",
    "        \"armageddon\",\n",
    "        \"artifact*\",\n",
    "        \"asteroid*\",\n",
    "        \"beaming\",\n",
    "        \"beast*\",\n",
    "        \"cosmic\",\n",
    "        \"creature*\",\n",
    "        \"extraterrestrial*\",\n",
    "        \"interstellar\",\n",
    "        \"lifeform*\",\n",
    "        \"meteor*\",\n",
    "        \"otherworldly\",\n",
    "        \"paranormal\",\n",
    "        \"prophecy\",\n",
    "        \"ritual*\",\n",
    "        \"sacred\",\n",
    "        \"saucer*\",\n",
    "        \"scripture*\",\n",
    "        \"spacecraft*\",\n",
    "        \"telepathy\",\n",
    "        \"ufo*\",\n",
    "        \"unidentified\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "#########\n",
    "# GAMES #\n",
    "#########\n",
    "\n",
    "# Define order\n",
    "ordered_games = [\n",
    "    \"Grand Theft Auto (1997)\",\n",
    "    \"Grand Theft Auto: London, 1969 (1999)\",\n",
    "    \"Grand Theft Auto: London, 1961 (1999)\",\n",
    "    \"Grand Theft Auto 2 (1999)\",\n",
    "    \"Grand Theft Auto III (2001)\",\n",
    "    \"Grand Theft Auto: Vice City (2002)\",\n",
    "    \"Grand Theft Auto: San Andreas (2004)\",\n",
    "    \"Grand Theft Auto Advance (2004)\",\n",
    "    \"Grand Theft Auto: Liberty City Stories (2005)\",\n",
    "    \"Grand Theft Auto: Vice City Stories (2006)\",\n",
    "    \"Grand Theft Auto IV (2008)\",\n",
    "    \"Grand Theft Auto IV: The Lost and Damned (2009)\",\n",
    "    \"Grand Theft Auto IV: The Ballad of Gay Tony (2009)\",\n",
    "    \"Grand Theft Auto: Chinatown Wars (2009)\",\n",
    "    \"Grand Theft Auto V (2013)\"\n",
    "]\n",
    "\n",
    "# Map abbreviations to games\n",
    "short_names_map = {\n",
    "    \"Grand Theft Auto (1997)\": \"GTA (1997)\",\n",
    "    \"Grand Theft Auto: London, 1969 (1999)\": \"GTA London, 1969 (1999)\",\n",
    "    \"Grand Theft Auto: London, 1961 (1999)\": \"GTA London, 1961 (1999)\",\n",
    "    \"Grand Theft Auto 2 (1999)\": \"GTA 2 (1999)\",\n",
    "    \"Grand Theft Auto III (2001)\": \"GTA III (2001)\",\n",
    "    \"Grand Theft Auto: Vice City (2002)\": \"GTA: VC (2002)\",\n",
    "    \"Grand Theft Auto: San Andreas (2004)\": \"GTA: SA (2004)\",\n",
    "    \"Grand Theft Auto Advance (2004)\": \"GTA Advance (2004)\",\n",
    "    \"Grand Theft Auto: Liberty City Stories (2005)\": \"GTA: LCS (2005)\",\n",
    "    \"Grand Theft Auto: Vice City Stories (2006)\": \"GTA: VCS (2006)\",\n",
    "    \"Grand Theft Auto IV (2008)\": \"GTA IV (2008)\",\n",
    "    \"Grand Theft Auto IV: The Lost and Damned (2009)\": \"GTA: TLAD (2009)\",\n",
    "    \"Grand Theft Auto IV: The Ballad of Gay Tony (2009)\": \"GTA: TBoGT (2009)\",\n",
    "    \"Grand Theft Auto: Chinatown Wars (2009)\": \"GTA: CTW (2009)\",\n",
    "    \"Grand Theft Auto V (2013)\": \"GTA V (2013)\"\n",
    "}\n",
    "\n",
    "# Map titles to folder names\n",
    "folder_mapping = {\n",
    "    \"gta\": \"Grand Theft Auto (1997)\",\n",
    "    \"gta 2\": \"Grand Theft Auto 2 (1999)\",\n",
    "    \"gta iii\": \"Grand Theft Auto III (2001)\",\n",
    "    \"gta vc\": \"Grand Theft Auto: Vice City (2002)\",\n",
    "    \"gta sa\": \"Grand Theft Auto: San Andreas (2004)\",\n",
    "    \"gta advance\": \"Grand Theft Auto Advance (2004)\",\n",
    "    \"gta lcs\": \"Grand Theft Auto: Liberty City Stories (2005)\",\n",
    "    \"gta vcs\": \"Grand Theft Auto: Vice City Stories (2006)\",\n",
    "    \"gta iv\": \"Grand Theft Auto IV (2008)\",\n",
    "    \"gta ctw\": \"Grand Theft Auto: Chinatown Wars (2009)\",\n",
    "    \"gta v\": \"Grand Theft Auto V (2013)\"\n",
    "}\n",
    "\n",
    "def get_game_label(path_string):\n",
    "    \"\"\"\n",
    "    Identifies game title from file path.\n",
    "    Handles DLC separation for GTA IV.\n",
    "    Maps titles to folder names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize paths\n",
    "    p = str(path_string).lower().replace('\\\\', '/')\n",
    "    \n",
    "    # Check for DLCs\n",
    "    if '1969' in p:\n",
    "        return \"Grand Theft Auto: London, 1969 (1999)\"\n",
    "    if '1961' in p:\n",
    "        return \"Grand Theft Auto: London, 1961 (1999)\"\n",
    "    if 'tlad' in p:\n",
    "        return \"Grand Theft Auto IV: The Lost and Damned (2009)\"\n",
    "    if 'tbogt' in p:\n",
    "        return \"Grand Theft Auto IV: The Ballad of Gay Tony (2009)\"\n",
    "    \n",
    "    # Extract folder names\n",
    "    parts = p.split('/')\n",
    "    \n",
    "    for part in parts:\n",
    "        if part in folder_mapping:\n",
    "            return folder_mapping[part]\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "############\n",
    "# CLEANING #\n",
    "############\n",
    "def is_technical_string(text):\n",
    "    \"\"\"\n",
    "    Filters technical artifacts.\n",
    "    \"\"\"\n",
    "    if '_' in text: return True\n",
    "    \n",
    "    if re.fullmatch(r'[\\d\\W_]+', text): return True\n",
    "    \n",
    "    if len(text) > 1 and text.isupper():\n",
    "        if ' ' in text:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_game_text(line, ext):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text based on file extension.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "\n",
    "    # Skip header\n",
    "    if not line or line in ['{', '}', 'Version'] or line.startswith(('CharSize', 'NeedDecode', 'SingleFileTable')):\n",
    "        return None\n",
    "    \n",
    "    text = None\n",
    "    if ext == '.oxt':\n",
    "        if '=' in line: text = line.split('=', 1)[1].strip()\n",
    "    elif ext == '.txt':\n",
    "        if line.startswith('[') and line.endswith(']'): return None\n",
    "        if '//' in line: text = line.split('//', 1)[1].strip()\n",
    "        else: text = line\n",
    "    elif ext == '.csv':\n",
    "        parts = re.findall(r'\"(.*?)\"', line)\n",
    "        if len(parts) >= 2:\n",
    "            if parts[0].lower() in ['gxt', 'id', 'key']: return None\n",
    "            text = parts[1]\n",
    "            \n",
    "    if text:\n",
    "        \n",
    "        # Remove markup\n",
    "        text = re.sub(r'~.*?~|#|<<|>>', '', text).strip()\n",
    "        \n",
    "        # Remove technical strings\n",
    "        if text and not is_technical_string(text):\n",
    "            return text if len(text) > 1 else None\n",
    "    return None\n",
    "\n",
    "\n",
    "#############\n",
    "# WILDCARDS #\n",
    "#############\n",
    "def wildcard_to_regex(wildcard_pattern):\n",
    "    \"\"\"\n",
    "    Converts search patterns into searchable regex objects.\n",
    "    \"\"\"\n",
    "    regex_str = r'\\b' + re.escape(wildcard_pattern).replace(r'\\*', r'\\w*') + r'\\b'\n",
    "    return re.compile(regex_str, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee73f1",
   "metadata": {},
   "source": [
    "# Decipher Files for London 1969 and London 1961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79384e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decipher(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Deciphers FXT files using a -1 ASCII shift.\n",
    "    Restructures text by forcing newlines before every opening bracket.\n",
    "    \"\"\"\n",
    "    input_file = Path(input_path)\n",
    "    output_file = Path(output_path)\n",
    "    \n",
    "    # Check existence of file\n",
    "    if not input_file.exists():\n",
    "        print(\"File not found.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        with open(input_file, 'rb') as f:\n",
    "            raw_bytes = f.read()\n",
    "        \n",
    "        # Apply shift\n",
    "        deciphered_chars = []\n",
    "        \n",
    "        for b in raw_bytes:\n",
    "            shifted_byte = (b - 1) % 256\n",
    "            char = chr(shifted_byte)\n",
    "            \n",
    "            # Structure text\n",
    "            if char == '[':\n",
    "                deciphered_chars.append('\\n')\n",
    "           \n",
    "            # Clean text\n",
    "            if 32 <= shifted_byte <= 126 or shifted_byte in [10, 13]:\n",
    "                deciphered_chars.append(char)\n",
    "            elif shifted_byte == 91:\n",
    "                deciphered_chars.append('[')\n",
    "        \n",
    "        final_text = \"\".join(deciphered_chars)\n",
    "        \n",
    "        # Export file\n",
    "        with open(output_file, 'w', encoding = 'utf-8') as f:\n",
    "            f.write(final_text)\n",
    "        \n",
    "        print(f\"File saved to '{output_file}'.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Execute\n",
    "# enguk.fxt  -> gta_london_1969.txt\n",
    "# enguke.fxt -> gta_london_1961.txt\n",
    "in_p = \"/Users/amb/Desktop/Corpus/speuk.FXT\" \n",
    "out_p = \"/Users/amb/Desktop/Corpus/speuk.txt\"\n",
    "\n",
    "# decipher(in_p, out_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789188f",
   "metadata": {},
   "source": [
    "# Describe Raw and Cleaned Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparative_audit(root_dir):\n",
    "    \"\"\"\n",
    "    Performs an audit to quantify noise reduction.\n",
    "    Measures lines, characters, tokens, and lexical diversity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize metric containers\n",
    "    raw_stats = {'lines': 0, 'chars': 0, 'tokens': []}\n",
    "    clean_stats = {'lines': 0, 'chars': 0, 'tokens': []}\n",
    "    total_files = 0\n",
    "    \n",
    "    root_path = Path(root_dir)\n",
    "    valid_extensions = {'.oxt', '.txt', '.csv'}\n",
    "    \n",
    "    # Collect data\n",
    "    for file_path in root_path.rglob('*'):\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext in valid_extensions:\n",
    "            total_files += 1\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "                    for line in f:\n",
    "                        raw_line = line.strip()\n",
    "                        if not raw_line: continue\n",
    "                        \n",
    "                        # Process raw data\n",
    "                        raw_stats['lines'] += 1\n",
    "                        raw_stats['chars'] += len(raw_line)\n",
    "                        raw_stats['tokens'].extend(re.findall(r'\\b\\w+\\b', raw_line.lower()))\n",
    "                        \n",
    "                        # Process cleaned data\n",
    "                        clean_text = extract_game_text(line, ext)\n",
    "                        if clean_text:\n",
    "                            clean_stats['lines'] += 1\n",
    "                            clean_stats['chars'] += len(clean_text)\n",
    "                            clean_stats['tokens'].extend(re.findall(r'\\b[a-zA-Z-]{2,}\\b', clean_text.lower()))\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calculate_metrics(s):\n",
    "        \"\"\"\n",
    "        Helps to calculate N (tokens), V (types), and TTR.\n",
    "        \"\"\"\n",
    "        n = len(s['tokens'])\n",
    "        v = len(set(s['tokens']))\n",
    "        ttr = v / n if n > 0 else 0\n",
    "        return [s['lines'], s['chars'], n, v, ttr]\n",
    "    \n",
    "    raw_vals = calculate_metrics(raw_stats)\n",
    "    clean_vals = calculate_metrics(clean_stats)\n",
    "    \n",
    "    # Calculate difference\n",
    "    diff_vals = [r - c for r, c in zip(raw_vals, clean_vals)]\n",
    "    \n",
    "    # Construct DataFrame\n",
    "    df_comp = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Lines', \n",
    "            'Characters', \n",
    "            'Tokens', \n",
    "            'Unique Word Types', \n",
    "            'Type-Token Ratio'\n",
    "        ],\n",
    "        'Raw': raw_vals,\n",
    "        'Cleaned': clean_vals,\n",
    "        'Difference': diff_vals\n",
    "    })\n",
    "    \n",
    "    print(f\"Audit complete. {total_files} files analyzed.\")\n",
    "    return df_comp\n",
    "\n",
    "# Execute and display\n",
    "audit_results = run_comparative_audit(base_path)\n",
    "\n",
    "display(audit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcadf9f",
   "metadata": {},
   "source": [
    "# Build Master Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a867fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_master_corpus(root_dir):\n",
    "    \"\"\"\n",
    "    Aggregates cleaned text from all files into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    root_path = Path(root_dir)\n",
    "    extensions = {'.oxt', '.txt', '.csv'}\n",
    "        \n",
    "    # Collect data\n",
    "    for file_path in root_path.rglob('*'):\n",
    "        ext = file_path.suffix.lower()\n",
    "        if ext in extensions:\n",
    "            \n",
    "            # Create clean relative paths\n",
    "            rel_path = str(file_path.relative_to(root_path)).replace('\\\\', '/')\n",
    "            \n",
    "            # Map game titles to paths\n",
    "            game_title = get_game_label(rel_path)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "                    for line in f:\n",
    "                        \n",
    "                        # Apply cleaning\n",
    "                        text = extract_game_text(line, ext)\n",
    "                        \n",
    "                        if text:\n",
    "                            all_data.append({\n",
    "                                'Game': game_title,\n",
    "                                'Path': rel_path,\n",
    "                                'Text': text\n",
    "                            })\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Construct DataFrame\n",
    "df_master = build_master_corpus(base_path)\n",
    "\n",
    "# Export file\n",
    "if not df_master.empty:\n",
    "    output_file = \"conspiracy_gta_master_corpus.csv\"\n",
    "    df_master.to_csv(output_file, index = False, encoding = 'utf-8-sig')\n",
    "    \n",
    "    print(f\"Corpus saved to '{output_file}'.\")\n",
    "    \n",
    "    # Display preview\n",
    "    display(df_master.head(25))\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a209766",
   "metadata": {},
   "source": [
    "# Search for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11abb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure search\n",
    "search_patterns = [\"fuck\"]\n",
    "export_results_file = \"conspiracy_gta_search_results.csv\"\n",
    "\n",
    "# Define search\n",
    "def perform_search(df, patterns):\n",
    "    \"\"\"\n",
    "    Scans the DataFrame for keywords and returns hits.\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    regex_list = [(p, wildcard_to_regex(p)) for p in patterns]\n",
    "        \n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row['Text'])\n",
    "        for original_pattern, regex in regex_list:\n",
    "            match = regex.search(text)\n",
    "            if match:\n",
    "                hits.append({\n",
    "                    'Game': row['Game'],\n",
    "                    'Pattern': original_pattern,\n",
    "                    'Match': match.group(0),\n",
    "                    'Context': text,\n",
    "                    'Path': row['Path']\n",
    "                })\n",
    "                \n",
    "                # Break after first match in line to avoid redundancy\n",
    "                break\n",
    "    \n",
    "    return pd.DataFrame(hits)\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals() and not df_master.empty:\n",
    "    df_hits = perform_search(df_master, search_patterns)\n",
    "    \n",
    "    if not df_hits.empty:\n",
    "        \n",
    "        # Export results\n",
    "        df_hits.to_csv(export_results_file, index = False, encoding = 'utf-8-sig')\n",
    "        \n",
    "        print(f\"{len(df_hits):,} matches found.\")\n",
    "        print(f\"Results exported to '{export_results_file}'.\")\n",
    "        \n",
    "        # Display preview\n",
    "        display(df_hits.head(25))\n",
    "    else:\n",
    "        print(\"No matches found.\")\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1350f",
   "metadata": {},
   "source": [
    "# Calculate Further Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caa0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advanced_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculates linguistic density metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect all words\n",
    "    word_pattern = re.compile(r'\\b[a-zA-Z-]{2,}\\b')\n",
    "    \n",
    "    # Create counters\n",
    "    word_counts = Counter()\n",
    "    line_word_counts = []\n",
    "    total_chars_in_words = 0\n",
    "    total_tokens = 0\n",
    "        \n",
    "    # Count\n",
    "    for text in df['Text']:\n",
    "        found_words = word_pattern.findall(str(text).lower())\n",
    "        \n",
    "        word_counts.update(found_words)\n",
    "        total_tokens += len(found_words)\n",
    "        line_word_counts.append(len(found_words))\n",
    "        \n",
    "        for w in found_words:\n",
    "            total_chars_in_words += len(w)\n",
    "    \n",
    "    # Calculate hapax legomena\n",
    "    hapax_list = [word for word, count in word_counts.items() if count == 1]\n",
    "    n_hapax = len(hapax_list)\n",
    "    vocabulary_size = len(word_counts)\n",
    "    \n",
    "    percentage_hapax = (n_hapax / vocabulary_size) * 100 if vocabulary_size > 0 else 0\n",
    "    \n",
    "    # Calculate average line length\n",
    "    avg_line_length = sum(line_word_counts) / len(line_word_counts) if line_word_counts else 0\n",
    "    \n",
    "    # Calculate average word length\n",
    "    avg_word_chars = total_chars_in_words / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    # Build result table\n",
    "    metrics = [\n",
    "        (\"Total Tokens\", total_tokens),\n",
    "        (\"Unique Types\", vocabulary_size),\n",
    "        (\"Hapax Legomena\", n_hapax),\n",
    "        (\"Percentage of Hapax Legomena\", f\"{percentage_hapax:.2f} %\"),\n",
    "        (\"Average Line Length (Words)\", f\"{avg_line_length:.2f}\"),\n",
    "        (\"Average Word Length (Characters)\", f\"{avg_word_chars:.2f}\")\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(metrics, columns = ['Metric', 'Value'])\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals():\n",
    "    df_adv_metrics = calculate_advanced_metrics(df_master)\n",
    "    display(df_adv_metrics)\n",
    "else:\n",
    "    print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe115a9",
   "metadata": {},
   "source": [
    "# Calculate Lexical Density, Display Top Bigrams, and Generate Word Length Distribution Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lexical_fingerprint(df):\n",
    "    \"\"\"\n",
    "    Analyzes bigrams, word length distribution, and lexical density.\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    # Pre-compile regex\n",
    "    word_pattern = re.compile(r'\\b[a-zA-Z-]{2,}\\b')\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Clean and tokenize line by line\n",
    "        line_tokens = word_pattern.findall(str(text).lower())\n",
    "        all_tokens.extend(line_tokens)\n",
    "        \n",
    "        # Calculate bigrams\n",
    "        if len(line_tokens) >= 2:\n",
    "            line_bigrams = zip(line_tokens, line_tokens[1:])\n",
    "            bigram_counts.update([f\"{a} {b}\" for a, b in line_bigrams])\n",
    "    \n",
    "    # Collect top 25 bigrams\n",
    "    top_bigrams = bigram_counts.most_common(25)\n",
    "    \n",
    "    # Calculate word lengths\n",
    "    word_lengths = [len(w) for w in all_tokens]\n",
    "    len_dist = Counter(word_lengths)\n",
    "    \n",
    "    # Calculate lexical density\n",
    "    stopwords = {\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\", \"then\", \"at\", \"by\", \n",
    "                 \"for\", \"with\", \"to\", \"of\", \"in\", \"on\", \"is\", \"was\", \"be\", \"it\", \"you\", \"that\"}\n",
    "    content_words_count = sum(1 for w in all_tokens if w not in stopwords)\n",
    "    lex_density = content_words_count / len(all_tokens) if all_tokens else 0\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Lexical density: {lex_density:.4f} (Ratio of content words to total words)\")\n",
    "    print(\"Top bigrams:\")\n",
    "    for phrase, count in top_bigrams:\n",
    "        print(f\"- \\\"{phrase}\\\": {count} times\")\n",
    "    \n",
    "    # Generate graph\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    # Filter for word lengths (2 to 12 characters)\n",
    "    plot_data = {length: len_dist[length] for length in range(2, 13)}\n",
    "    \n",
    "    plt.bar(plot_data.keys(), plot_data.values(), color = '#2c3e50', edgecolor = 'white', alpha = 0.8)\n",
    "    \n",
    "    plt.title('Word Length Distribution', fontsize = 14, pad = 15)\n",
    "    plt.xlabel('Characters per Word', fontsize = 11)\n",
    "    plt.ylabel('Frequency', fontsize = 11)\n",
    "    \n",
    "    plt.xticks(range(2, 13))\n",
    "    plt.xlim(1.5, 12.5)\n",
    "    \n",
    "    plt.grid(axis = 'y', linestyle = '--', alpha = 0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('conspiracy_gta_word_length_distribution.png', dpi = 300)\n",
    "    \n",
    "    print(\"Graph saved as 'conspiracy_gta_word_length_distribution.png'.\")\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals():\n",
    "    run_lexical_fingerprint(df_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8acbc6",
   "metadata": {},
   "source": [
    "# Generate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab21448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_heatmap(df):\n",
    "    \"\"\"\n",
    "    Creates a normalized heatmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate totals\n",
    "    game_totals = df['Game'].value_counts()\n",
    "    \n",
    "    # Get categories\n",
    "    category_order = list(conspiracy_categories.keys())\n",
    "    \n",
    "    # Prepare regex objects\n",
    "    compiled_cats = {cat: [wildcard_to_regex(p) for p in patterns] for cat, patterns in conspiracy_categories.items()}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for cat, regexes in compiled_cats.items():\n",
    "        \n",
    "        # Combine all patterns of a category into one regex\n",
    "        combined_pattern = \"|\".join([r.pattern for r in regexes])\n",
    "        combined_regex = re.compile(combined_pattern, re.IGNORECASE)\n",
    "        \n",
    "        # Search for hits\n",
    "        mask = df['Text'].str.contains(combined_regex, na = False)\n",
    "        hits = df.loc[mask, 'Game']\n",
    "        \n",
    "        for game in hits:\n",
    "            results.append({'Game': game, 'Category': cat})\n",
    "    \n",
    "    df_hits = pd.DataFrame(results)\n",
    "    \n",
    "    # Fallback if nothing is found\n",
    "    if df_hits.empty:\n",
    "        print(\"No matches found for any category.\")\n",
    "        return\n",
    "    \n",
    "    # Create matrix\n",
    "    matrix_abs = df_hits.groupby(['Game', 'Category']).size().unstack(fill_value = 0)\n",
    "    \n",
    "    # Normalize\n",
    "    matrix_rel = matrix_abs.divide(game_totals, axis = 0) * 1000\n",
    "    \n",
    "    # Handle zero hits\n",
    "    matrix_rel = matrix_rel.reindex(index = ordered_games, columns = category_order).fillna(0)\n",
    "    \n",
    "    # Generate graph\n",
    "    plt.figure(figsize = (16, 10))\n",
    "    \n",
    "    sns.heatmap(matrix_rel, annot = True, fmt = '.2f', cmap = 'YlGnBu',\n",
    "                cbar_kws={'label': 'Hits per 1,000 Lines of a Game'}, linewidths = .5)\n",
    "    \n",
    "    plt.xlabel('Thematic Dimension', fontsize = 12, labelpad = 15)\n",
    "    plt.ylabel('Game', fontsize = 12, labelpad = 15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('conspiracy_gta_heatmap.png', dpi = 300)\n",
    "    \n",
    "    print(\"Graph saved as 'conspiracy_gta_heatmap.png'.\")\n",
    "    display(matrix_rel)\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals() and not df_master.empty:\n",
    "    run_heatmap(df_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8534c",
   "metadata": {},
   "source": [
    "# Generate Barcode for Thematic Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_barcode_dimensions(df, categories):\n",
    "    \"\"\"\n",
    "    Creates a barcode of conspiracy categories for thematic dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Order\n",
    "    df_filtered = df[df['Game'].isin(ordered_games)].copy()\n",
    "    df_filtered['Sort_Order'] = df_filtered['Game'].apply(lambda x: ordered_games.index(x))\n",
    "    df_sorted = df_filtered.sort_values('Sort_Order').reset_index(drop = True)\n",
    "    \n",
    "    all_words = []\n",
    "    game_boundaries = []\n",
    "    current_game = None\n",
    "    \n",
    "    for i, row in df_sorted.iterrows():\n",
    "        game = row['Game']\n",
    "        if game != current_game:\n",
    "            game_boundaries.append({'start': len(all_words), 'name': game})\n",
    "            current_game = game\n",
    "        \n",
    "        tokens = re.findall(r'\\b[a-zA-Z-]{2,}\\b', str(row['Text']).lower())\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    total_len = len(all_words)\n",
    "    \n",
    "    # Set up graph\n",
    "    category_names = list(categories.keys())\n",
    "    fig, ax = plt.subplots(figsize = (22, 10))\n",
    "    colors = ['#ffffff', '#f7f7f7']\n",
    "    \n",
    "    # Draw background and labels\n",
    "    for i, boundary in enumerate(game_boundaries):\n",
    "        start = boundary['start']\n",
    "        end = game_boundaries[i + 1]['start'] if i + 1 < len(game_boundaries) else total_len\n",
    "        short_name = short_names_map.get(boundary['name'], boundary['name'])\n",
    "        \n",
    "        ax.axvspan(start, end, facecolor = colors[i % 2], alpha = 1.0, zorder = 0)\n",
    "        ax.axvline(x = end, color = '#dddddd', lw = 0.8, zorder = 1)\n",
    "        \n",
    "        depth_steps = [-0.05, -0.1, -0.15, -0.2, -0.25, -0.3, -0.35, -0.4, -0.45, -0.5]\n",
    "        \n",
    "        if i < 9:\n",
    "            drop_depth = depth_steps[i]\n",
    "        else:\n",
    "            drop_depth = depth_steps[-1]\n",
    "        \n",
    "        mid = start + (end - start) / 2\n",
    "        \n",
    "        ax.annotate('', xy = (mid, 0), xycoords = ('data', 'axes fraction'),\n",
    "                    xytext = (mid, drop_depth), textcoords = ('data', 'axes fraction'),\n",
    "                    arrowprops = dict(arrowstyle = '-', color = '#999999', lw = 1))\n",
    "        \n",
    "        ax.text(mid, drop_depth - 0.02, short_name, transform = ax.get_xaxis_transform(),\n",
    "                rotation = 45, ha = 'right', va = 'top', fontsize = 10, fontweight = 'bold')\n",
    "\n",
    "    # Draw bars\n",
    "    for idx, (cat_name, patterns) in enumerate(reversed(list(categories.items()))):\n",
    "        # Combine all patterns of a category into one search\n",
    "        combined_indices = []\n",
    "        for pattern_str in patterns:\n",
    "            regex = wildcard_to_regex(pattern_str)\n",
    "            indices = [j for j, word in enumerate(all_words) if regex.match(word)]\n",
    "            combined_indices.extend(indices)\n",
    "        \n",
    "        # Plot all hits for a category as vertical lines\n",
    "        ax.vlines(combined_indices, idx - 0.35, idx + 0.35,\n",
    "                  colors = '#003366', alpha = 0.5, linewidth = 0.4, zorder = 2)\n",
    "    \n",
    "    # Set style\n",
    "    ax.set_yticks(range(len(category_names)))\n",
    "    ax.set_yticklabels(reversed(category_names), fontsize = 12, fontweight = 'bold')\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    ax.set_xlabel('Line in Corpus', fontsize = 13, labelpad = 160)\n",
    "    \n",
    "    plt.subplots_adjust(bottom = 0.3)\n",
    "    plt.savefig('conspiracy_gta_barcode_dimensions.png', dpi = 300, bbox_inches = 'tight')\n",
    "    \n",
    "    print(\"Graph saved as 'conspiracy_gta_barcode_dimensions.png'.\")\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals():\n",
    "    run_barcode_dimensions(df_master, conspiracy_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a7cda",
   "metadata": {},
   "source": [
    "# Generate Barcode for Selected Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_barcode_words(df, target_words):\n",
    "    \"\"\"\n",
    "    Creates a barcode of conspiracy categories by keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    # Order\n",
    "    df_filtered = df[df['Game'].isin(ordered_games)].copy()\n",
    "    df_filtered['Sort_Order'] = df_filtered['Game'].apply(lambda x: ordered_games.index(x))\n",
    "    df_sorted = df_filtered.sort_values('Sort_Order').reset_index(drop = True)\n",
    "    \n",
    "    all_words = []\n",
    "    game_boundaries = []\n",
    "    current_game = None\n",
    "    \n",
    "    for i, row in df_sorted.iterrows():\n",
    "        game = row['Game']\n",
    "        if game != current_game:\n",
    "            game_boundaries.append({'start': len(all_words), 'name': game})\n",
    "            current_game = game\n",
    "        \n",
    "        tokens = re.findall(r'\\b[a-zA-Z-]{2,}\\b', str(row['Text']).lower())\n",
    "        all_words.extend(tokens)\n",
    "    \n",
    "    total_len = len(all_words)\n",
    "    \n",
    "    # Set up graph\n",
    "    fig, ax = plt.subplots(figsize = (22, 10))\n",
    "    colors = ['#ffffff', '#f7f7f7']\n",
    "    \n",
    "    # Draw background and labels\n",
    "    for i, boundary in enumerate(game_boundaries):\n",
    "        start = boundary['start']\n",
    "        end = game_boundaries[i + 1]['start'] if i + 1 < len(game_boundaries) else total_len\n",
    "        short_name = short_names_map.get(boundary['name'], boundary['name'])\n",
    "        \n",
    "        ax.axvspan(start, end, facecolor = colors[i % 2], alpha = 1.0, zorder = 0)\n",
    "        ax.axvline(x = end, color = '#dddddd', lw = 0.8, zorder = 1)\n",
    "        \n",
    "        depth_steps = [-0.05, -0.1, -0.15, -0.2, -0.25, -0.3, -0.35, -0.4, -0.45, -0.5]\n",
    "        \n",
    "        if i < 9:\n",
    "            drop_depth = depth_steps[i]\n",
    "        else:\n",
    "            drop_depth = depth_steps[-1]\n",
    "        \n",
    "        mid = start + (end - start) / 2\n",
    "        \n",
    "        ax.annotate('', xy = (mid, 0), xycoords = ('data', 'axes fraction'),\n",
    "                    xytext = (mid, drop_depth), textcoords = ('data', 'axes fraction'),\n",
    "                    arrowprops = dict(arrowstyle = '-', color = '#999999', lw = 1))\n",
    "        \n",
    "        ax.text(mid, drop_depth - 0.02, short_name, transform = ax.get_xaxis_transform(),\n",
    "                rotation = 45, ha = 'right', va = 'top', fontsize = 10, fontweight = 'bold')\n",
    "\n",
    "    # Draw bars\n",
    "    for idx, word in enumerate(reversed(target_words)):\n",
    "        pattern = wildcard_to_regex(word)\n",
    "        indices = [j for j, w in enumerate(all_words) if pattern.match(w)]\n",
    "        \n",
    "        # Plot all hits for a keyword as vertical lines\n",
    "        ax.vlines(indices, idx - 0.35, idx + 0.35,\n",
    "                  colors = '#003366', alpha = 0.6, linewidth = 0.5, zorder = 2)\n",
    "    \n",
    "    # Set style\n",
    "    ax.set_yticks(range(len(target_words)))\n",
    "    ax.set_yticklabels(reversed(target_words), fontsize = 12, fontweight = 'bold')\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    ax.set_xlabel('Line in Corpus', fontsize = 13, labelpad = 160)\n",
    "    \n",
    "    plt.subplots_adjust(bottom = 0.3)\n",
    "    plt.savefig('conspiracy_gta_barcode_keywords.png', dpi = 300, bbox_inches = 'tight')\n",
    "    \n",
    "    print(\"Graph saved as 'conspiracy_gta_barcode_keywords.png'.\")\n",
    "\n",
    "# Set keywords\n",
    "barcode_keywords = [\"money\", \"kill\", \"fuck*\", \"bureau*\", \"clon*\", \"brainwash*\", \"ufo*\"]\n",
    "\n",
    "# Execute\n",
    "if 'df_master' in locals():\n",
    "    run_barcode_words(df_master, barcode_keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
